# -*- coding: utf-8 -*-
"""Pattern_Proj.ipynb
# Rahij Gillani git
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/154m17Am1LKFDkLnHQTryOe-m4kZj0DPT
"""

import math
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import matplotlib.patches as mpatches
from scipy.stats import norm
from numpy.linalg import norm, inv
import matplotlib.pyplot as plt 
import matplotlib.tri as tri
from sklearn.metrics import confusion_matrix, mean_squared_error
from numpy import cov
import tensorflow as tf
from statistics import stdev, mean 
import csv
from copy import deepcopy
from pprint import pprint
import random
from sklearn.model_selection import RepeatedKFold
from sklearn import preprocessing
from sklearn import svm
from sklearn.svm import LinearSVC
from sklearn import metrics
from sklearn.decomposition import PCA
from sklearn.preprocessing import scale
from sklearn.neighbors import (NeighborhoodComponentsAnalysis, KNeighborsClassifier)
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from mpl_toolkits.mplot3d import Axes3D  
import time
from sklearn.ensemble import RandomForestRegressor

X_train = pd.read_csv('C:\\Users\\rahij\\Desktop\\Waterloo\\Pattern\\UCI HAR Dataset\\train\\X_train.txt', header=None, delim_whitespace=True)
y_train = pd.read_csv('C:\\Users\\rahij\\Desktop\\Waterloo\\Pattern\\UCI HAR Dataset\\train\\y_train.txt', header=None)
y_train = np.ravel(np.array(y_train))
X_train = np.array(X_train)

X_test = pd.read_csv('C:\\Users\\rahij\\Desktop\\Waterloo\\Pattern\\UCI HAR Dataset\\test\\X_test.txt', header=None, delim_whitespace=True)
y_test = pd.read_csv('C:\\Users\\rahij\\Desktop\\Waterloo\\Pattern\\UCI HAR Dataset\\test\\y_test.txt', header=None)
y_test = np.ravel(np.array(y_test))
X_test = np.array(X_test)

X = np.append(X_train, X_test, axis=0)
y = np.append(y_train, y_test)

"""# **PCA**"""

XX_train = scale(X_train)
print(XX_train.shape)
pca = PCA()
pca.fit(X_train)
v = np.cumsum(np.round(pca.explained_variance_ratio_, decimals=3)*100);
for i, j in enumerate(v):
  if round(j) == 99:
    print("99% of variance in data occurs in first " + str(i+1) + " componennts" )
    break

plt.ylabel('% Variance Explained')
plt.xlabel('# of Features')
plt.title('PCA Analysis')
plt.ylim(30,100.5)
plt.plot(v)
plt.show()
pca = PCA(n_components=99)

"""# **Weighted K-NN**"""

rkf = RepeatedKFold(n_splits=10, n_repeats=10)
count = 0
acc = []
print('Calculating 10X10 fold accurcay for Weighted K-NN with different # of neighbours')
for i in range(1,50,1):#50
  neigh = KNeighborsClassifier(n_neighbors=i, weights='distance', algorithm='kd_tree')
  count+=1
  print('neighbours= ',count)  
  n_list = []
  for train_index, test_index in rkf.split(X_train):

    train_data, test_data = X_train[train_index, :], X_train[test_index, :]    

    r = pca.fit_transform(train_data, y=None)
    neigh.fit(r, y_train[train_index])
    XX_test = pca.transform(test_data)
    re = neigh.predict(XX_test)
    n_list.append(metrics.accuracy_score(y_train[test_index], re))

  acc.append(mean(n_list))

plt.ylabel('Accuracy')
plt.xlabel('# of Neighbours')
plt.title('K-NN')
plt.plot(list(range(1,50,1)),acc)
plt.plot()
plt.show()
print("Maximum accurcay is " + str(acc[np.argmax(np.array(acc))]) + 
      " with " + str(np.argmax(np.array(acc))+1) + " neighbours.")

neigh = KNeighborsClassifier(n_neighbors=4, weights='distance', algorithm='kd_tree')
r = pca.fit_transform(X_train, y=None)

neigh.fit(r, y_train)
start_time = time.time()

XX_test = pca.transform(X_test)

re = neigh.predict(XX_test)
print("Test Accuracy: ",metrics.accuracy_score(y_test, re))
print("--- %s seconds to predict ---" % (time.time() - start_time))

"""# **Neighbourhood Component Analysis (NCA)**"""

# Accuracy vs # of Dimensions with k=4 neighbours
print('Plotting Accuracy vs Dimensions for k=4 neighbours')
nca_list = []
for i in range(1,20,1):
  print('Dimensions =', i)
  nca = NeighborhoodComponentsAnalysis(n_components=i,random_state=42, warm_start=True)
  nca.fit(X_train, y_train)
  neigh = KNeighborsClassifier(n_neighbors=4, weights='distance', algorithm='kd_tree')
  neigh.fit(nca.transform(X_train), y_train)
  re = neigh.predict(nca.transform(X_test))
  nca_list.append(metrics.accuracy_score(y_test, re))

plt.ylabel('Accuracy')
plt.xlabel('# of Dimensions')
plt.title('NCA+K-NN')
plt.plot(list(range(1,20,1)),nca_list)
plt.show()
print("Maximum accurcay is " + str(nca_list[np.argmax(np.array(nca_list))]) + 
      " with " + str(np.argmax(np.array(nca_list))+1) + " components.")

print("Plotting Accuracy vs # of neighbours with Dimensions = 5")
nca = NeighborhoodComponentsAnalysis(n_components=5, random_state=42, warm_start=True)
nca.fit(X_train, y_train)
nca_neigh_list = []
for i in range(1,200,1):
  neigh = KNeighborsClassifier(n_neighbors=i, weights='distance', algorithm='kd_tree')
  neigh.fit(nca.transform(X_train), y_train)

  re = neigh.predict(nca.transform(X_test))
  nca_neigh_list.append(metrics.accuracy_score(y_test, re))

plt.ylabel('Accuracy')
plt.xlabel('# of Neighbours')
plt.title('NCA+K-NN')
plt.plot(list(range(1,200,1)),nca_neigh_list)
plt.plot()
plt.show()
print("Maximum accurcay is " + str(nca_neigh_list[np.argmax(np.array(nca_neigh_list))]) + 
      " with " + str(np.argmax(np.array(nca_neigh_list))+1) + " neighbours.")

"""# **Validation**"""

rkf = RepeatedKFold(n_splits=10, n_repeats=10)
count = 0
acc = []
print('Calculating 10X10 fold accurcay for NCA+K-NN')
nca = NeighborhoodComponentsAnalysis(n_components=5,random_state=42, warm_start=True)
for train_index, test_index in rkf.split(X_train):
  count+=1
  print(count)
  train_data, test_data = X_train[train_index, :], X_train[test_index, :]

  nca.fit(train_data, y_train[train_index])
  neigh = KNeighborsClassifier(n_neighbors=16, weights='distance', algorithm='kd_tree')
  neigh.fit(nca.transform(train_data), y_train[train_index])
  re = neigh.predict(nca.transform(test_data))
  acc.append(metrics.accuracy_score(y_train[test_index], re))

print("10x10 Fold Accuracy NCA")
print("Testing Accuracy: ",mean(acc))

nca = NeighborhoodComponentsAnalysis(n_components=5,random_state=42, warm_start=True)
nca.fit(X_train, y_train)
neigh = KNeighborsClassifier(n_neighbors=16, weights='distance', algorithm='kd_tree')
neigh.fit(nca.transform(X_train), y_train)

start_time = time.time()
re = neigh.predict(nca.transform(X_test))
print("Test Accuracy: ",metrics.accuracy_score(y_test, re))
print("--- %s seconds to predict---" % (time.time() - start_time))

print('Confusion Matrix')
print(confusion_matrix(y_test,re))

"""# **SVM**"""

rkf = RepeatedKFold(n_splits=10, n_repeats=10)
count = 0
acc_svm = []
print('10x10 fold Svm')
for train_index, test_index in rkf.split(X_train):
  count+=1
  print(count)
  train_data, test_data = X_train[train_index, :], X_train[test_index, :]

  svm_model_linear = svm.SVC(kernel = 'linear', C = 1).fit(train_data, y_train[train_index]) 
  svm_predictions = svm_model_linear.predict(test_data) 
  acc_svm.append(metrics.accuracy_score(y_train[test_index], svm_predictions))

print("10x10 Fold Accuracy SVM")
print("Accuracy: ",mean(acc_svm))

svm_model_linear = svm.SVC(kernel = 'linear', C = 1).fit(X_train, y_train) 
start_time = time.time()

svm_predictions = svm_model_linear.predict(X_test) 
print("Test Accuracy: ",metrics.accuracy_score(y_test, svm_predictions))
print("--- %s seconds to predict---" % (time.time() - start_time))

"""# **PCA vs LDA vs NCA**"""

print('Plotting Comparison plots PCA vs LDA vs NCA')
# Reduce to 2 Dimensions with PCA
s = StandardScaler()
pca = PCA(n_components=2, random_state=42)
#pca = Pipeline([('scaler', s), ('pca', pc)])

# Reduce to 2 Dimensions with PCA with LinearDiscriminantAnalysis
lda = LinearDiscriminantAnalysis(n_components=2)
#lda = Pipeline([('scaler', s),('lda', ld) ])

# Reduce to 2 Dimensions with PCA with NeighborhoodComponentAnalysis
nca = NeighborhoodComponentsAnalysis(n_components=2, random_state=42)
#nca = Pipeline([('scaler', s), ('nca', nc)])

# Use a nearest neighbor classifier to evaluate the methods
knn = KNeighborsClassifier(n_neighbors=16, weights='distance', algorithm='kd_tree')

# Make a list of the methods to be compared
dim_reduction_methods = [('PCA', pca), ('LDA', lda), ('NCA', nca)]

# plt.figure()
for i, (name, model) in enumerate(dim_reduction_methods):
    fig = plt.figure()
    #fig.add_subplot(111, projection='3d')
    # plt.subplot(1, 3, i + 1, aspect=1)

    # Fit the method's model
    model.fit(X_train, y_train)

    # Fit a nearest neighbor classifier on the embedded training set
    knn.fit(model.transform(X_train), y_train)

    # Compute the nearest neighbor accuracy on the embedded test set
    acc_knn = knn.score(model.transform(X_test), y_test)

    # Embed the data set in 2 dimensions using the fitted model
    X_embedded = model.transform(X)

    # Plot the projected points and show the evaluation score
    plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y,  cmap='Set1')
    plt.title("{}, KNN (k={})\nTest accuracy = {:.2f}".format(name,
                                                              16,
                                                              acc_knn))
plt.show()